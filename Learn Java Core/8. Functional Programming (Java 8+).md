### **Lesson 1: Introduction to Functional Programming & Lambdas**

#### **1. Concept Explanation**

##### **What is Functional Programming (FP)?**
Functional Programming is a programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It's a **declarative** style of programming ("what to do") rather than an **imperative** style ("how to do it").

| Imperative (OOP) | Declarative (FP) |
| :--- | :--- |
| **How:** "Create a loop, initialize a counter, iterate from 0 to 9, get the element at the current index, check if it's even, if so, add it to a new list." | **What:** "Give me a list of all the even numbers from the original list." |

**Core Tenets of FP:**
*   **Pure Functions:** A function's output depends only on its input arguments, and it has no observable side effects (like modifying a global variable, writing to a file, etc.).
*   **Immutability:** Data structures are not modified after they are created. "Changes" result in the creation of new data structures.
*   **First-Class Functions:** Functions are treated like any other variable. They can be passed as arguments to other functions, returned as values from functions, and stored in data structures. This is the cornerstone of how FP is implemented in Java.

##### **Lambda Expressions: Functions as Variables**
A **lambda expression** is an anonymous (unnamed) function. It allows you to treat functionality as a method argument, or code as data. Lambdas are Java's way of enabling "first-class functions."

**Anatomy of a Lambda Expression:**
```
(parameter1, parameter2) -> { // code block }
```
1.  **Parameters:** `(parameter1, parameter2)` - A list of parameters for the function. Type inference often allows you to omit the types.
2.  **Arrow Token:** `->` - Separates the parameters from the body.
3.  **Body:** `{ // code block }` - The implementation of the function. If the body is a single expression, you can omit the curly braces and the `return` keyword.

**Evolution from Anonymous Inner Class to Lambda:**
Before Java 8, if you wanted to pass behavior, you had to use a verbose anonymous inner class.

```java
// Pre-Java 8: Anonymous Inner Class
Runnable r1 = new Runnable() {
    @Override
    public void run() {
        System.out.println("Hello from anonymous class!");
    }
};

// Java 8: Lambda Expression
// The compiler knows 'r2' is a Runnable, which has one abstract method, run(), that takes no arguments.
Runnable r2 = () -> System.out.println("Hello from lambda!");
```

##### **Capturing Variables: The "Effectively Final" Rule**
A lambda expression can access variables from its enclosing scope (the surrounding method). This is called "capturing." However, there's a strict rule: a lambda can only access local variables that are **final** or **effectively final**.

*   **Effectively Final:** A variable is effectively final if its value is never changed after it is initialized.

**Why this rule?** This rule exists to prevent concurrency issues and maintain a clear contract. If a lambda could modify a local variable, it would create complex problems if the lambda were executed on a different thread long after the original method has finished. By enforcing the "effectively final" rule, Java ensures that the lambda is only capturing a fixed *value*, not a mutable variable.

##### **Method References**
A method reference is a shorthand, more readable syntax for a lambda expression that only calls a single, existing method. There are four main types:

1.  **Static Method Reference:** `ClassName::staticMethodName`
    *   `str -> Integer.parseInt(str)` becomes `Integer::parseInt`
2.  **Instance Method Reference (on a specific instance):** `instance::instanceMethodName`
    *   `() -> myPrinter.println()` becomes `myPrinter::println`
3.  **Instance Method Reference (on an arbitrary object of a type):** `ClassName::instanceMethodName`
    *   `(s1, s2) -> s1.compareToIgnoreCase(s2)` becomes `String::compareToIgnoreCase`
4.  **Constructor Reference:** `ClassName::new`
    *   `() -> new ArrayList<>()` becomes `ArrayList::new`

---

#### **2. Example Code Snippet**

```java
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

public class LambdasAndMethodRefsDemo {
    public static void main(String[] args) {
        List<String> names = Arrays.asList("alice", "bob", "charlie");

        // --- 1. Imperative Style (Pre-Java 8) ---
        List<String> upperCaseNamesImperative = new ArrayList<>();
        for (String name : names) {
            upperCaseNamesImperative.add(name.toUpperCase());
        }
        System.out.println("Imperative: " + upperCaseNamesImperative);


        // --- 2. Functional Style with Lambda ---
        // Using the Stream API, which we'll cover in detail later.
        // The lambda 'name -> name.toUpperCase()' is passed as behavior.
        List<String> upperCaseNamesLambda = names.stream()
                                                 .map(name -> name.toUpperCase())
                                                 .toList(); // .toList() is from Java 16+
        System.out.println("Lambda:     " + upperCaseNamesLambda);


        // --- 3. Functional Style with Method Reference ---
        // 'String::toUpperCase' is a more readable version of the lambda above.
        List<String> upperCaseNamesMethodRef = names.stream()
                                                    .map(String::toUpperCase)
                                                    .toList();
        System.out.println("Method Ref: " + upperCaseNamesMethodRef);


        // --- Capturing Variables ---
        String prefix = "User: ";
        // The lambda 'name -> System.out.println(prefix + name)' captures 'prefix'.
        // This is allowed because 'prefix' is effectively final.
        names.forEach(name -> System.out.println(prefix + name));
        
        // If you try to change prefix, the code will not compile:
        // prefix = "Guest: "; // COMPILE ERROR
    }
}
```

---

#### **3. Mini Exercise**

You have a `List<String>`.
1.  Sort the list alphabetically using `list.sort()`. Provide a lambda expression for the comparison logic.
2.  Now, sort the same list by the length of the strings, from shortest to longest.
3.  Finally, sort the list by length again, but this time in descending order (longest to shortest).
4.  For the first step (alphabetical sort), try to rewrite the lambda `(s1, s2) -> s1.compareTo(s2)` using a method reference.

---

#### **4. Quiz Question**

**Question:** Which of the following is **not** a valid lambda expression?

A) `() -> {}`
B) `s -> s.length()`
C) `(int x, int y) -> return x + y;`
D) `(x, y) -> x + y`

*(Scroll down for the answer)*

...

**Answer:** C) `(int x, int y) -> return x + y;`. When a lambda's body is a single expression, you cannot use curly braces `{}` or the `return` keyword. It should be written as in D: `(x, y) -> x + y`. If you want to use the `return` keyword, you must enclose the body in curly braces: `(int x, int y) -> { return x + y; }`.

### **Lesson 3: Stream API Fundamentals**

#### **1. Concept Explanation**

##### **Stream vs. Collection: A Conceptual Shift**
This is the most important concept to grasp. A `Stream` is **not** a data structure. A `Collection` is a data structure; it is an in-memory container for data.

*   A **`Collection`** is about **data**. You can add to it, remove from it, and ask for its size.
*   A **`Stream`** is about **computation**. It is a sequence of elements from a source that supports aggregate operations. It represents a "view" of the data, not the data itself.

**Analogy:**
*   A **`Collection`** is like a **bottle of water**. All the water (data) is physically present in the bottle.
*   A **`Stream`** is like a **river**. The water (data) flows through it. You can place filters, turbines, and taps (operations) along the river to process the water as it passes by, but the river itself doesn't store the water.

**Key Differences:**
1.  **No Storage:** Streams do not store elements.
2.  **Immutable:** A stream pipeline does not modify its underlying data source (the collection). Filtering a stream produces a new stream with the filtered results; it doesn't remove elements from the original collection.
3.  **Lazy Evaluation:** This is the key to a stream's performance. Intermediate operations are not executed immediately. They are only executed when a terminal operation is invoked.
4.  **Possibly Infinite:** Because they are lazy, streams can represent infinite sequences of data (e.g., a stream of all prime numbers). A collection cannot.
5.  **Consumable (Single-Use):** A stream can only be traversed once. After a terminal operation is called, the stream is "consumed" and cannot be reused.

##### **The Stream Pipeline Structure**
Every stream operation follows a three-stage pipeline:
**Source → Intermediate Operation(s) → Terminal Operation**

```
  +------------------+
  |      Source      |  (e.g., a List, an Array, a file)
  +------------------+
          |
          v
+--------------------+
|  filter(...)       |  <-- Intermediate Operation (lazy)
+--------------------+
          |
          v
+--------------------+
|  map(...)          |  <-- Intermediate Operation (lazy)
+--------------------+
          |
          v
+--------------------+
|  collect(...)      |  <-- Terminal Operation (triggers execution)
+--------------------+
          |
          v
  +------------------+
  |      Result      |  (e.g., a new List, a count, a single value)
  +------------------+
```
1.  **Source:** Where the stream gets its elements. This can be a `Collection`, an array, a generator function, or an I/O channel.
2.  **Intermediate Operations:** These are operations that transform a stream into another stream. They are always **lazy**. They don't perform any processing until a terminal operation is called. Examples include `filter`, `map`, `sorted`, `distinct`.
3.  **Terminal Operation:** This is the operation that triggers the execution of the entire pipeline and produces a result or a side effect. After the terminal operation is complete, the stream cannot be used again. Examples include `collect`, `forEach`, `reduce`, `count`.

##### **Lazy Evaluation and Internal Iteration in Action**
Consider this code:
```java
List<String> names = Arrays.asList("Alice", "Bob", "Charlie", "Anna");

names.stream()
     .filter(s -> {
         System.out.println("Filtering: " + s);
         return s.startsWith("A");
     })
     .map(s -> {
         System.out.println("Mapping: " + s);
         return s.toUpperCase();
     })
     .forEach(s -> System.out.println("ForEach: " + s));
```
A common misconception is that this code will first filter all names, then map all the filtered names, and then print them. This is **incorrect**.

Because of lazy evaluation, the stream processes the elements **vertically**, not horizontally. The `forEach` terminal operation pulls elements one by one through the entire pipeline.

**Actual Execution Flow:**
1.  `forEach` asks for an element.
2.  `map` asks `filter` for an element.
3.  `filter` takes "Alice" from the source. It passes the filter (`Filtering: Alice`).
4.  `filter` sends "Alice" to `map`.
5.  `map` transforms "Alice" to "ALICE" (`Mapping: Alice`).
6.  `map` sends "ALICE" to `forEach`.
7.  `forEach` prints "ALICE" (`ForEach: ALICE`).
---
8.  `forEach` asks for the next element.
9.  `map` asks `filter` for an element.
10. `filter` takes "Bob". It fails the filter test (`Filtering: Bob`).
11. `filter` asks the source for the next element.
---
12. `filter` takes "Charlie". It fails the filter test (`Filtering: Charlie`).
13. `filter` asks the source for the next element.
---
14. `filter` takes "Anna". It passes the filter (`Filtering: Anna`).
15. `filter` sends "Anna" to `map`.
16. `map` transforms "Anna" to "ANNA" (`Mapping: Anna`).
17. `map` sends "ANNA" to `forEach`.
18. `forEach` prints "ANNA" (`ForEach: ANNA`).

---

#### **2. Example Code Snippet: Creating Streams**

```java
import java.util.Arrays;
import java.util.List;
import java.util.stream.IntStream;
import java.util.stream.Stream;

public class StreamCreationDemo {
    public static void main(String[] args) {
        // 1. From a Collection
        List<String> list = Arrays.asList("a", "b", "c");
        Stream<String> streamFromList = list.stream();
        System.out.print("From List: ");
        streamFromList.forEach(s -> System.out.print(s + " "));

        // 2. From an Array
        String[] array = {"x", "y", "z"};
        Stream<String> streamFromArray = Arrays.stream(array);
        System.out.print("\nFrom Array: ");
        streamFromArray.forEach(s -> System.out.print(s + " "));

        // 3. From individual values
        Stream<Integer> streamOfValues = Stream.of(1, 2, 3, 4, 5);
        System.out.print("\nFrom Values: ");
        streamOfValues.forEach(i -> System.out.print(i + " "));

        // 4. An infinite stream using iterate()
        // Creates a stream of even numbers: 0, 2, 4, 6, ...
        Stream<Integer> infiniteStream = Stream.iterate(0, n -> n + 2);
        System.out.print("\nFirst 5 even numbers from infinite stream: ");
        // We MUST use a short-circuiting operation like limit() on an infinite stream.
        infiniteStream.limit(5).forEach(i -> System.out.print(i + " "));

        // 5. Primitive stream (avoids boxing)
        IntStream intStream = IntStream.range(1, 4); // Range is exclusive for the end
        System.out.print("\nFrom IntStream.range(1, 4): ");
        intStream.forEach(i -> System.out.print(i + " "));
    }
}
```

---

#### **3. Mini Exercise**

You have a `List<String>`.
1.  Create a stream from this list.
2.  Chain two intermediate operations:
    *   `filter()` to keep only the strings that have a length greater than 3.
    *   `limit()` to keep only the first 2 elements that pass the filter.
3.  Add a terminal operation `forEach()` to print the final resulting strings.
4.  Before the terminal operation, add a `peek(s -> System.out.println("Inspecting: " + s))` operation after the `filter` to see which elements are being processed.

---

#### **4. Quiz Question**

**Question:** What happens if you try to use a stream after a terminal operation has already been called on it?

A) The stream will automatically reset and can be reused.
B) The stream will re-run the pipeline with the same result.
C) It will throw an `IllegalStateException`.
D) It will result in a `NullPointerException`.

*(Scroll down for the answer)*

...

**Answer:** C) It will throw an `IllegalStateException`. Streams are single-use. Once a terminal operation is invoked, the stream is considered "consumed" or "closed," and any further attempt to operate on it will result in an `IllegalStateException`.

### **Lesson 4: Intermediate and Terminal Stream Operations**

#### **1. Concept Explanation**

This lesson dives into the "verbs" of the Stream API—the operations you can perform on the data as it flows through the pipeline.

##### **Intermediate Operations (Lazy and Stream-in, Stream-out)**

These operations are the building blocks of your data transformation pipeline. Each one takes a stream and returns a new stream, allowing them to be chained together. They do not execute until a terminal operation is called.

*   **`filter(Predicate<T> predicate)`:**
    *   **Purpose:** To filter elements from a stream. It keeps only the elements that return `true` for the given predicate.
    *   **Example:** `stream.filter(s -> s.startsWith("A"))`

*   **`map(Function<T, R> mapper)`:**
    *   **Purpose:** To transform each element of a stream into another object. The input and output types can be different. It's a one-to-one mapping.
    *   **Example:** `stream.map(String::length)` transforms a `Stream<String>` into a `Stream<Integer>`.

*   **`flatMap(Function<T, Stream<R>> mapper)`:**
    *   **Purpose:** A powerful but sometimes confusing operation. It is used to "flatten" a stream of streams into a single stream. It maps each element to a `Stream`, and then concatenates all those individual streams into one.
    *   **Analogy:** `map` is like giving a book to each person in a list. `flatMap` is like asking each person in a list for their *collection* of books, and then putting all the books from all the collections onto a single, flat shelf.
    *   **Example:** `List<List<Integer>> listOfLists; listOfLists.stream().flatMap(list -> list.stream())` transforms a `Stream<List<Integer>>` into a single `Stream<Integer>`.

*   **`distinct()`:**
    *   **Purpose:** Returns a stream consisting of the distinct elements of the stream. It uses the `equals()` method of the elements to determine uniqueness.
    *   **Example:** `Stream.of(1, 2, 1, 3, 2).distinct()` results in a stream of `1, 2, 3`.

*   **`sorted()` & `sorted(Comparator<T> comparator)`:**
    *   **Purpose:** Sorts the elements of the stream. `sorted()` uses the natural order (`Comparable`), while `sorted(comparator)` uses a custom `Comparator`.
    *   **Note:** This is a **stateful** intermediate operation. It may need to see all the elements before it can produce a result.

*   **`limit(long maxSize)` & `skip(long n)`:**
    *   **Purpose:** `limit` truncates the stream to be no longer than `maxSize`. `skip` discards the first `n` elements.
    *   **Note:** These are **short-circuiting** operations. For example, `limit(5)` will stop processing after it has found 5 elements, which can be a significant performance optimization.

*   **`peek(Consumer<T> action)`:**
    *   **Purpose:** Primarily for debugging. It allows you to perform an action on each element as it flows through the pipeline, without changing the element.
    *   **Example:** `stream.peek(System.out::println)` will print each element.

##### **Terminal Operations (Eager and Produces a Result)**

These operations trigger the execution of the entire pipeline and produce a final result. The stream is consumed after a terminal operation.

*   **`forEach(Consumer<T> action)`:**
    *   **Purpose:** Performs an action for each element of the stream. This is typically used for side effects, like printing to the console.

*   **`collect(Collector collector)`:**
    *   **Purpose:** The most powerful and common terminal operation. It transforms the elements of the stream into a different form, typically a `Collection` (like a `List` or `Set`) or a `Map`. We will cover `Collector`s in depth later.
    *   **Example:** `stream.collect(Collectors.toList())`

*   **`reduce(...)`:**
    *   **Purpose:** Performs a reduction on the elements of the stream, using an associative accumulation function, and returns a single result.
    *   **Example:** `numbers.stream().reduce(0, (a, b) -> a + b)` sums all numbers, starting with an identity of 0.

*   **`count()`:**
    *   **Purpose:** Returns the count of elements in the stream as a `long`.

*   **Matching Operations (Short-circuiting):**
    *   **`anyMatch(Predicate<T> predicate)`:** Returns `true` if at least one element matches the predicate.
    *   **`allMatch(Predicate<T> predicate)`:** Returns `true` if all elements match the predicate.
    *   **`noneMatch(Predicate<T> predicate)`:** Returns `true` if no elements match the predicate.

*   **Finding Operations:**
    *   **`findFirst()`:** Returns an `Optional` describing the first element of the stream.
    *   **`findAny()`:** Returns an `Optional` describing some element of the stream. In parallel streams, this can be more efficient than `findFirst`.

---

#### **2. Example Code Snippet: A Data Processing Pipeline**

This example uses a list of `Transaction` objects and chains several operations to find a specific result.

```java
import java.util.Arrays;
import java.util.Comparator;
import java.util.List;
import java.util.stream.Collectors;

class Transaction {
    private final int id;
    private final int value;
    private final String city;

    public Transaction(int id, int value, String city) {
        this.id = id;
        this.value = value;
        this.city = city;
    }

    public int getValue() { return value; }
    public String getCity() { return city; }

    @Override
    public String toString() { return "Transaction{id=" + id + ", value=" + value + ", city='" + city + "'}"; }
}

public class StreamOperationsDemo {
    public static void main(String[] args) {
        List<Transaction> transactions = Arrays.asList(
            new Transaction(1, 100, "London"),
            new Transaction(2, 500, "Milan"),
            new Transaction(3, 800, "London"),
            new Transaction(4, 250, "Tokyo"),
            new Transaction(5, 500, "London")
        );

        // PROBLEM: Find the IDs of the top 2 transactions by value from London, sorted by value.

        List<Integer> result = transactions.stream() // 1. Get the stream
            .filter(t -> "London".equals(t.getCity()))   // 2. Keep only London transactions
            .sorted(Comparator.comparing(Transaction::getValue).reversed()) // 3. Sort by value, descending
            .limit(2)                                    // 4. Take only the top 2
            .map(t -> t.id)                              // 5. Map from Transaction -> transaction ID (this part of the original requirement is not in the code. so I am adding it)
            .collect(Collectors.toList());               // 6. Collect the results into a List

        System.out.println("IDs of the top 2 transactions from London:");
        System.out.println(result);
    }
}
```

---

#### **3. Mini Exercise**

You have a `List<String> words = Arrays.asList("Java", "Stream", "API", "is", "very", "powerful", "java");`
Write a stream pipeline that:
1.  Makes all words lowercase.
2.  Removes any duplicate words.
3.  Keeps only the words that are shorter than 5 characters.
4.  Sorts the remaining words alphabetically.
5.  Collects the final result into a `List<String>`.
6.  Print the final list.

---

#### **4. Quiz Question**

**Question:** What is the fundamental difference between `map()` and `flatMap()`?

A) `map()` is an intermediate operation, while `flatMap()` is a terminal operation.
B) `map()` performs a one-to-one transformation (one input element produces one output element), while `flatMap()` performs a one-to-many transformation and flattens the results into a single stream.
C) `map()` can change the type of the stream, but `flatMap()` cannot.
D) `map()` is lazy, while `flatMap()` is eager.

*(Scroll down for the answer)*

...

**Answer:** B) `map()` performs a one-to-one transformation (one input element produces one output element), while `flatMap()` performs a one-to-many transformation and flattens the results into a single stream. `map(f)` applies the function `f` to each element, resulting in a new stream of the same size. `flatMap(f)` applies the function `f` to each element, where `f` returns a stream for each element, and then all of those resulting streams are merged into one.

### **Lesson 5: The `Optional` Class**

#### **1. Concept Explanation**

##### **The "Billion-Dollar Mistake": `null`**
Tony Hoare, the inventor of `null` references, famously called it his "billion-dollar mistake." `null` is problematic because it's ambiguous. Does a `null` return value mean "the value was not found," "the computation failed," or "the value is undefined"? It forces the client of an API to constantly check for `null` to avoid the dreaded `NullPointerException`.

```java
// The old way: error-prone null checking
User user = findUserById(id);
if (user != null) {
    // Nested null checks can get very messy
    if (user.getAddress() != null) {
        if (user.getAddress().getCity() != null) {
            System.out.println(user.getAddress().getCity());
        }
    }
}
```

##### **The Solution: `Optional<T>`**
Java 8 introduced the `java.util.Optional<T>` class to provide a better, more explicit way to handle the absence of a value. An `Optional` is a container object that may or may not contain a non-null value.

*   **It is a wrapper:** It either contains an object of type `T` (it is "present") or it is empty.
*   **It is an explicit contract:** When a method returns an `Optional<User>`, it is explicitly telling the client: "This method might not find a user. You must be prepared to handle the case where the value is absent." This forces the developer to think about the "not found" case, turning a potential runtime `NullPointerException` into a compile-time design consideration.

##### **Creating `Optional`s**
*   **`Optional.of(T value)`:** Creates an `Optional` with the given value. **Important:** The value *must not* be `null`. If it is, this method will throw a `NullPointerException`. Use this when you are certain the value is non-null.
*   **`Optional.ofNullable(T value)`:** Creates an `Optional` that wraps the value if it's non-null, or returns an empty `Optional` if the value is `null`. This is the safest and most common way to create an `Optional` from a value that might be null.
*   **`Optional.empty()`:** Returns a singleton empty `Optional` instance.

##### **Using `Optional`s (Avoiding `get()`)**
The most common mistake beginners make is to immediately call `optional.get()`. If the `Optional` is empty, `get()` will throw a `NoSuchElementException`, which is no better than a `NullPointerException`.

**The `Optional` API is designed to be used in a functional, declarative style.**

*   **`isPresent()` & `isEmpty()` (Java 11+):** Checks if a value is present. `isPresent()` should generally be avoided in favor of more functional methods.

*   **Providing Alternatives (Consuming the value):**
    *   **`orElse(T other)`:** Returns the contained value if present, otherwise returns the provided default value `other`.
    *   **`orElseGet(Supplier<? extends T> other)`:** Returns the contained value if present, otherwise returns the result of invoking the provided `Supplier`. This is more efficient than `orElse` if creating the default object is expensive, as the `Supplier` is only invoked when needed.
    *   **`orElseThrow(Supplier<? extends X> exceptionSupplier)`:** Returns the contained value if present, otherwise throws the exception created by the provided `Supplier`. This is the standard way to signal that an absent value is an error.

*   **Performing Actions:**
    *   **`ifPresent(Consumer<? super T> consumer)`:** If a value is present, it executes the given `Consumer` with the value, otherwise does nothing.

*   **Functional Transformations:**
    *   **`map(Function<? super T, ? extends U> mapper)`:** If a value is present, it applies the mapping function to it and returns an `Optional` describing the result. If the `Optional` is empty, it returns an empty `Optional`. This allows for safe chaining.
    *   **`flatMap(Function<? super T, Optional<U>> mapper)`:** Similar to `map`, but the mapping function itself must return an `Optional`. This is used to "flatten" nested `Optional`s, avoiding `Optional<Optional<User>>`.

---

#### **2. Example Code Snippet: Refactoring Null Checks**

```java
import java.util.HashMap;
import java.util.Map;
import java.util.Optional;

class UserProfile {
    // A method that might return null in the old world
    public String getCity_Old() {
        // ... logic that might result in null ...
        return null;
    }

    // The modern way: explicitly state that the value may be absent
    public Optional<String> getCity_New() {
        return Optional.empty(); // or Optional.ofNullable(someValue);
    }
}

public class OptionalDemo {
    private static Map<Integer, String> userDb = new HashMap<>();
    static {
        userDb.put(1, "Alice");
    }

    // A repository method that correctly returns an Optional
    public static Optional<String> findUsernameById(int id) {
        return Optional.ofNullable(userDb.get(id));
    }

    public static void main(String[] args) {
        // --- Consuming an Optional ---
        System.out.println("--- Handling presence and absence ---");
        // Case 1: User found
        String username1 = findUsernameById(1)
                            .orElse("Guest"); // The default value is not used
        System.out.println("User 1: " + username1);

        // Case 2: User not found
        String username2 = findUsernameById(99)
                            .orElse("Guest"); // The default value "Guest" is returned
        System.out.println("User 99: " + username2);

        // Case 3: Throwing an exception if not found
        try {
            String username3 = findUsernameById(99)
                                .orElseThrow(() -> new IllegalStateException("User not found"));
        } catch (IllegalStateException e) {
            System.out.println("Caught expected exception: " + e.getMessage());
        }

        // --- Chaining with map() ---
        System.out.println("\n--- Chaining with map() ---");
        // We want to get the length of the username if it exists.
        Optional<Integer> nameLength = findUsernameById(1)
                                         .map(String::length); // map() safely transforms the value

        nameLength.ifPresent(len -> System.out.println("Username length is: " + len));
    }
}
```

---

#### **3. Mini Exercise**

You have a `Map<String, String> config` that might contain a key `"port"`.
1.  Write a piece of code that retrieves the port number from this map.
2.  If the `"port"` key exists, its value (a `String`) should be parsed into an `Integer`.
3.  If the key does not exist, or if the value is not a valid number, you should use a default port of `8080`.
4.  Implement this logic using `Optional`.
    *   Start with `Optional.ofNullable(config.get("port"))`.
    *   Use `flatMap` to safely attempt to parse the string to an integer (Hint: `s -> { try { return Optional.of(Integer.parseInt(s)); } catch (NumberFormatException e) { return Optional.empty(); } }`).
    *   Use `orElse` to provide the default value.

---

#### **4. Quiz Question**

**Question:** What is the difference between `optional.orElse(createDefault())` and `optional.orElseGet(() -> createDefault())`?

A) There is no difference; they are functionally identical.
B) `orElse` is for providing a default value, while `orElseGet` is for throwing an exception.
C) The `createDefault()` method is **always** executed when using `orElse`, even if the optional is present. With `orElseGet`, the supplier lambda `() -> createDefault()` is **only** executed if the optional is empty.
D) `orElse` can return `null`, but `orElseGet` cannot.

*(Scroll down for the answer)*

...

**Answer:** C) The `createDefault()` method is **always** executed when using `orElse`, even if the optional is present. With `orElseGet`, the supplier lambda `() -> createDefault()` is **only** executed if the optional is empty. This makes `orElseGet` the preferred choice when the creation of the default object is a computationally expensive operation.

### **Lesson 6: Collectors and the `collect` Operation**

#### **1. Concept Explanation**

`collect` is the most powerful and flexible terminal operation in the Stream API. While other terminal operations like `count()` or `forEach()` perform a specific task, `collect` is a general-purpose operation that can transform the elements of a stream into almost any data structure you need.

The behavior of the `collect` method is defined by a `java.util.stream.Collector`.

##### **The `Collectors` Utility Class**
You will rarely implement the `Collector` interface yourself. Instead, you will almost always use the numerous static factory methods provided in the `java.util.stream.Collectors` class. These pre-defined collectors cover a wide range of common use cases.

##### **1. Collecting to a Collection**
These are the most common collectors.
*   **`toList()`:** Collects stream elements into a `java.util.List`.
*   **`toSet()`:** Collects stream elements into a `java.util.Set`, automatically removing duplicates.
*   **`toCollection(Supplier<C> collectionFactory)`:** Collects elements into a specific type of collection, e.g., `toCollection(LinkedList::new)`.

##### **2. Collecting to a String**
*   **`joining()`:** Concatenates the stream elements into a single `String`.
*   **`joining(CharSequence delimiter)`:** Concatenates elements with a specified delimiter.
*   **`joining(CharSequence delimiter, CharSequence prefix, CharSequence suffix)`:** Adds a prefix and suffix.

##### **3. Aggregation and Summarization (Reduction)**
These collectors perform a reduction operation on the stream.
*   **`counting()`:** Counts the number of elements in the stream.
*   **`summingInt(ToIntFunction)` / `summingLong(...)` / `summingDouble(...)`:** Calculates the sum of a property of the stream elements.
*   **`averagingInt(ToIntFunction)` / `averagingDouble(...)`:** Calculates the average.
*   **`summarizingInt(ToIntFunction)` / `summarizingDouble(...)`:** A powerful collector that calculates the count, sum, min, max, and average all in one pass. It returns a special summary object (e.g., `IntSummaryStatistics`).

##### **4. Grouping Operations**
These are the most advanced and powerful collectors, allowing you to perform "GROUP BY" style operations.

*   **`groupingBy(Function classifier)`:** Groups the elements of a stream into a `Map`. The keys of the map are the result of applying the `classifier` function, and the values are `List`s of the elements that mapped to that key.
    *   **Example:** `students.stream().collect(groupingBy(Student::getGrade))` would produce a `Map<Grade, List<Student>>`.

*   **Downstream Collectors:** The `groupingBy` method has an overloaded version that takes a second `Collector` as an argument. This "downstream collector" is applied to the results of each group. This allows for multi-level grouping and complex aggregations.
    *   **Example:** `students.stream().collect(groupingBy(Student::getGrade, counting()))` would produce a `Map<Grade, Long>` that maps each grade to the *count* of students in that grade.

*   **`partitioningBy(Predicate predicate)`:** A special case of `groupingBy` that is optimized for partitioning elements into two groups based on a predicate. It always returns a `Map<Boolean, List<T>>`, where `true` is the key for the list of elements that match the predicate, and `false` is the key for the list of elements that do not.

---

#### **2. Example Code Snippet**

This example uses a `List<Dish>` to demonstrate various powerful collectors.

```java
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;
import static java.util.stream.Collectors.*; // Static import for readability

class Dish {
    private final String name;
    private final boolean vegetarian;
    private final int calories;
    private final Type type;
    enum Type { MEAT, FISH, OTHER }
    // Constructor, getters, toString...
    public Dish(String name, boolean veg, int cal, Type type) { this.name=name; this.vegetarian=veg; this.calories=cal; this.type=type;}
    public String getName() { return name; }
    public boolean isVegetarian() { return vegetarian; }
    public int getCalories() { return calories; }
    public Type getType() { return type; }
    @Override public String toString() { return name; }
}

public class CollectorsDemo {
    public static void main(String[] args) {
        List<Dish> menu = List.of(
            new Dish("pork", false, 800, Dish.Type.MEAT),
            new Dish("beef", false, 700, Dish.Type.MEAT),
            new Dish("chicken", false, 400, Dish.Type.MEAT),
            new Dish("french fries", true, 530, Dish.Type.OTHER),
            new Dish("rice", true, 350, Dish.Type.OTHER),
            new Dish("salad", true, 120, Dish.Type.OTHER),
            new Dish("salmon", false, 450, Dish.Type.FISH)
        );

        // 1. joining(): Create a comma-separated list of all dish names
        String allNames = menu.stream()
                              .map(Dish::getName)
                              .collect(joining(", "));
        System.out.println("All dishes: " + allNames);

        // 2. summingInt(): Calculate the total calories of all dishes
        int totalCalories = menu.stream()
                                .collect(summingInt(Dish::getCalories));
        System.out.println("Total calories: " + totalCalories);

        // 3. groupingBy(): Group dishes by their type
        Map<Dish.Type, List<Dish>> dishesByType = menu.stream()
                                                      .collect(groupingBy(Dish::getType));
        System.out.println("Dishes by type: " + dishesByType);

        // 4. groupingBy() with a downstream collector: Count dishes in each group
        Map<Dish.Type, Long> dishCountByType = menu.stream()
                                                   .collect(groupingBy(Dish::getType, counting()));
        System.out.println("Dish count by type: " + dishCountByType);

        // 5. partitioningBy(): Partition dishes into vegetarian and non-vegetarian
        Map<Boolean, List<Dish>> vegetarianDishes = menu.stream()
                                                        .collect(partitioningBy(Dish::isVegetarian));
        System.out.println("Vegetarian partition: " + vegetarianDishes);
    }
}
```

---

#### **3. Mini Exercise**

You are given a `List<String> sentences = List.of("hello world", "java stream api", "flatmap is powerful");`.
Your task is to produce a `Map<String, Long>` that counts the frequency of each unique word across all sentences.
1.  Start with `sentences.stream()`.
2.  You will need to split each sentence into words. The method `s -> Arrays.stream(s.split(" "))` can be used for this. Which intermediate operation should you use to flatten the resulting `Stream<Stream<String>>` into a single `Stream<String>` of words?
3.  Use a `groupingBy` collector. The classifier function should be the word itself (`Function.identity()`).
4.  The downstream collector should be `counting()`.
5.  Print the final map of word counts.

---

#### **4. Quiz Question**

**Question:** You have a `Stream<Employee>` and you want to create a `Map<Department, Set<Employee>>` where employees are grouped by their department, and the values are sets of employees to eliminate duplicates within each department. Which collector would you use?

A) `Collectors.toMap(Employee::getDepartment, employee -> employee)`
B) `Collectors.groupingBy(Employee::getDepartment)`
C) `Collectors.partitioningBy(e -> e.getDepartment() != null)`
D) `Collectors.groupingBy(Employee::getDepartment, Collectors.toSet())`

*(Scroll down for the answer)*

...

**Answer:** D) `Collectors.groupingBy(Employee::getDepartment, Collectors.toSet())`. The standard `groupingBy(classifier)` collects the grouped elements into a `List`. To change this behavior, you provide a downstream collector. In this case, `Collectors.toSet()` is used as the downstream collector to gather the employees for each department into a `Set` instead of a `List`.

### **Lesson 7: Parallel Streams and Performance**

#### **1. Concept Explanation**

##### **Sequential vs. Parallel Streams**
By default, all stream operations in Java are **sequential**. A sequential stream is processed by a single thread on a single CPU core.
A **parallel stream** is a stream that is designed to be processed by multiple threads concurrently, automatically leveraging multiple CPU cores.

The Stream API makes parallelism incredibly easy to access:
*   To get a parallel stream, you can call **`collection.parallelStream()`** instead of `collection.stream()`.
*   You can also convert an existing stream to parallel by calling the intermediate operation **`.parallel()`**.
*   You can convert a parallel stream back to sequential with `.sequential()`.

##### **How Parallel Streams Work Under the Hood**
Parallel streams are built on top of the **Fork/Join Framework** (which we covered in the Concurrency topic).
1.  **Splitting:** The stream source is recursively split into smaller subproblems. This is done by a `Spliterator`, an internal iterator that can be easily split.
2.  **Processing:** Each subproblem is then submitted as a task to the common **`ForkJoinPool`**.
3.  **Joining:** The `ForkJoinPool`'s worker threads process the subproblems in parallel. If a thread finishes its work, it "steals" work from another thread's queue. The results from the subproblems are then combined in the final step.

**Diagram of Parallel Execution:**
```
+-----------------------------------+
|      Original Data Source         |
+-----------------------------------+
             | (split)
     +-----------------------+
     |   Chunk A   |   Chunk B   |
     +-----------------------+
             | (split)
+----------+----------+----------+----------+
| Chunk A1 | Chunk A2 | Chunk B1 | Chunk B2 |
+----------+----------+----------+----------+
     |          |          |          |
     v          v          v          v
+--------+  +--------+  +--------+  +--------+
| Thread 1|  | Thread 2|  | Thread 3|  | Thread 4|  <-- ForkJoinPool
+--------+  +--------+  +--------+  +--------+
(process A1) (process A2) (process B1) (process B2)
     |          |          |          |
     |        (combine)       |        |
     +-----------------------+
             | (combine)
             v
+-----------------------------------+
|          Final Result             |
+-----------------------------------+
```

##### **When to Use Parallel Streams (And When Not To)**
Turning a stream parallel with `.parallel()` is seductively easy, but it is **not** a magic performance boost. Using it incorrectly can make your application significantly *slower* and cause hard-to-debug concurrency bugs.

**Use Parallel Streams When:**
1.  You have a **large amount of data** to process (tens of thousands of elements or more). The overhead of splitting and coordinating threads is only worth it for large datasets.
2.  The processing for each element is **CPU-intensive**. Parallelism helps when the CPU is the bottleneck.
3.  The operations in your pipeline are **independent** and **stateless**. The work on one element should not depend on another.
4.  The data source splits easily (e.g., `ArrayList`, arrays).

**AVOID Parallel Streams When:**
1.  **The dataset is small.** The overhead of the Fork/Join framework will be greater than the benefit of parallelism.
2.  The tasks are **I/O-bound** (e.g., making network calls, reading from a database). A parallel stream will just cause many threads to block on I/O, wasting resources. For I/O-bound tasks, asynchronous approaches with `CompletableFuture` or virtual threads are far superior.
3.  Your stream operations depend on order (e.g., `limit()`). Some operations are much more expensive in a parallel context.
4.  You are modifying **shared mutable state** from within your lambdas. This is the biggest danger.

##### **The Danger: Shared Mutable State**
If your lambda expressions modify a shared object or collection, a parallel stream will introduce a massive race condition. The different threads will be trying to update the shared state concurrently without any synchronization, leading to data corruption.

**Rule:** The lambdas used in stream pipelines, especially parallel ones, should be **stateless and free of side effects**.

---

#### **2. Example Code Snippet: Performance Comparison**

This example shows a CPU-intensive task where a parallel stream provides a clear performance benefit.

```java
import java.util.stream.LongStream;

public class ParallelStreamDemo {

    public static void main(String[] args) {
        long limit = 50_000_000L; // A large number of elements

        // --- 1. Sequential Sum ---
        long startTime = System.currentTimeMillis();
        long sequentialSum = LongStream.rangeClosed(1, limit)
                                       .sum();
        long endTime = System.currentTimeMillis();
        System.out.println("Sequential Sum: " + sequentialSum);
        System.out.println("Sequential Time: " + (endTime - startTime) + " ms");

        // --- 2. Parallel Sum ---
        startTime = System.currentTimeMillis();
        long parallelSum = LongStream.rangeClosed(1, limit)
                                     .parallel() // This is the only change needed!
                                     .sum();
        endTime = System.currentTimeMillis();
        System.out.println("Parallel Sum:   " + parallelSum);
        System.out.println("Parallel Time:   " + (endTime - startTime) + " ms");
        
        // On a multi-core machine, the parallel time will be significantly less.
    }
}
```

##### **Example of a Dangerous Side Effect:**
```java
List<Integer> unsafeList = new ArrayList<>();
IntStream.range(0, 1000).parallel().forEach(i -> {
    unsafeList.add(i); // DANGER: Race condition! Multiple threads adding to a non-thread-safe list.
});
// The final size of unsafeList will be unpredictable and less than 1000.
// The correct way is to use a collector: .collect(Collectors.toList());
```

---

#### **3. Mini Exercise**

You have a list of one million `Double` values. Your task is to perform a complex, CPU-intensive calculation on each one and then find the sum of the results.
1.  Create a `List<Double>` and populate it with one million random doubles.
2.  Create a static method `double performComplexCalculation(double input)` that simulates a heavy workload (e.g., `Math.log(Math.sqrt(Math.abs(input)))` or just a `Thread.sleep(1)` for simulation, although sleeping is not CPU-intensive).
3.  First, implement the solution using a sequential stream: `myList.stream().mapToDouble(MyClass::performComplexCalculation).sum()`. Time its execution.
4.  Second, implement the solution using a parallel stream: `myList.parallelStream()...`. Time its execution.
5.  Compare the timings.

---

#### **4. Quiz Question**

**Question:** Under which of the following conditions would using a parallel stream likely make your program *slower*?

A) Performing a simple `sum()` on a large `long[]` array.
B) Mapping a list of 10 million `String`s to their uppercase versions.
C) Processing a list of 50 URLs where each processing step involves making a network request to download the URL's content.
D) Filtering a large `ArrayList` of integers to find prime numbers.

*(Scroll down for the answer)*

...

**Answer:** C) Processing a list of 50 URLs where each processing step involves making a network request to download the URL's content. This is an **I/O-bound** task. The default `ForkJoinPool` used by parallel streams has a limited number of threads (typically equal to the number of CPU cores). All these threads will quickly become blocked waiting for network responses, and the system won't be able to process the URLs in parallel effectively. This is a classic case where asynchronous tools like `CompletableFuture` are the correct choice.

### **Lesson 8: Topic Summary, Interview Questions, and Final Project**

This final lesson consolidates our in-depth exploration of functional programming in Java. We'll summarize the key concepts, review common interview questions, and apply everything in a practical data processing project.

---

#### **1. Summary Table of Key Concepts**

| Concept | Description & Key Takeaway | Example Syntax |
| :--- | :--- | :--- |
| **Lambda Expression** | An anonymous function that can be treated as a value. Enables "first-class functions." | `(params) -> expression` |
| **Functional Interface** | An interface with exactly one abstract method. The target type for a lambda. Use `@FunctionalInterface`. | `Predicate<T>`, `Function<T, R>`, `Consumer<T>`, `Supplier<T>` |
| **Method Reference** | A compact shorthand for a lambda that calls a single, existing method. | `String::toUpperCase`, `System.out::println`, `ArrayList::new` |
| **Stream** | A sequence of elements from a source that supports aggregate operations. It's about **computation**, not data storage. Streams are lazy and single-use. | `myList.stream()` |
| **Stream Pipeline**| **Source → Intermediate Operations → Terminal Operation.** Intermediate operations are lazy; the terminal operation triggers execution. | `stream.filter(...).map(...).collect(...)` |
| **Intermediate Ops** | `filter`, `map`, `flatMap`, `sorted`, `distinct`, `limit`. Return a new stream. | `.filter(s -> s.length() > 3)` |
| **Terminal Ops** | `collect`, `forEach`, `reduce`, `count`, `findFirst`, `anyMatch`. Produce a result and consume the stream. | `.collect(Collectors.toList())` |
| **`Collectors`** | A utility class providing powerful `Collector` implementations for use with `stream.collect()`. | `toList()`, `toSet()`, `joining()`, `groupingBy()`, `summingInt()` |
| **`Optional<T>`** | A container object for a value that may be absent. Used to explicitly handle the absence of a value and avoid `NullPointerException`. | `optional.orElse("default")`, `optional.map(...)` |
| **Parallel Stream** | A stream that processes elements concurrently using the common `ForkJoinPool`. Use for large, CPU-intensive, and stateless operations. | `myList.parallelStream()` |
| **Immutability & Purity** | FP encourages immutable data and pure functions (no side effects) for safer, more predictable code, especially in parallel contexts. | Lambdas in streams should not modify external state. |

---

#### **2. Common Interview Questions**

1.  **"What is the difference between a Stream and a Collection?"**
    *   *Answer:* A Collection is an in-memory data structure that stores values. A Stream is a view over a data source that allows you to express computations. Streams don't store data, are lazy, and can only be traversed once.

2.  **"Explain the difference between `map` and `flatMap` in the Stream API."**
    *   *Answer:* `map` performs a one-to-one transformation; it takes one element and produces one element in the output stream. `flatMap` performs a one-to-many transformation; it takes one element, maps it to a *new stream* of zero or more elements, and then flattens all the resulting streams into a single output stream. It's used to "un-nest" structures.

3.  **"What does it mean for a stream operation to be 'lazy'?"**
    *   *Answer:* Laziness means that intermediate operations in a stream pipeline are not executed immediately. They are only executed when a terminal operation is invoked. This allows the Stream API to perform optimizations, such as combining operations (loop fusion) and short-circuiting (e.g., `findFirst` stopping after the first match).

4.  **"What is the 'effectively final' rule for variables used in lambdas?"**
    *   *Answer:* A lambda expression can only access local variables from its enclosing scope if those variables are `final` or "effectively final." A variable is effectively final if its value is never changed after initialization. This rule exists to prevent concurrency issues and ensure that the lambda captures a fixed value, not a mutable variable.

5.  **"When should you use `Optional`? What is a common mistake when using it?"**
    *   *Answer:* You should use `Optional` as the return type for any method that might not be able to return a value, to make this possibility explicit in the method's contract. It forces the caller to handle the "absent" case. A common mistake is to immediately call `.get()` without checking `isPresent()` first, or to write `if (opt.isPresent()) { return opt.get(); } else { return null; }`, which defeats the entire purpose of `Optional`. The preferred way is to use functional methods like `orElse()`, `map()`, and `ifPresent()`.

6.  **"Is using `.parallelStream()` always faster? When can it make performance worse?"**
    *   *Answer:* No, it is not always faster. It can make performance worse if the dataset is small (the overhead of forking/joining is too high), if the task is I/O-bound (threads will just block), or if the underlying data structure does not split efficiently (like a `LinkedList`). It can also cause incorrect results if the lambda operations have side effects that modify shared mutable state.

---

#### **3. Final Mini-Project: Log File Analytics Pipeline**

This project will apply functional programming concepts to read, parse, and analyze a log file to generate a summary report.

**🎯 Goal:** Build a log processing pipeline using Streams and Collectors to extract meaningful statistics from a set of log entries.

**Project Components:**

1.  **`LogEntry` Record:**
    *   Create a Java `record` (or a simple POJO) to represent a single log entry:
        `record LogEntry(LocalDateTime timestamp, String level, String message) {}`
    *   `level` will be "INFO", "WARN", or "ERROR".

2.  **Log Parser:**
    *   Create a static method `Optional<LogEntry> parseLogLine(String line)`.
    *   This method will take a raw log string (e.g., `"2023-10-27T10:00:00,INFO,User logged in"`) and attempt to parse it into a `LogEntry` object.
    *   If the line is malformed (e.g., doesn't split correctly), it should **return an empty `Optional`**. Otherwise, it should return an `Optional` containing the created `LogEntry`.

3.  **Data Source:**
    *   Create a `List<String>` of sample log lines. Include a mix of log levels and some malformed lines that your parser will reject.
        ```java
        List<String> rawLogs = List.of(
            "2023-10-27T10:00:00,INFO,User logged in",
            "2023-10-27T10:01:00,INFO,Data processed successfully",
            "2023-10-27T10:02:00,WARN,Connection timing out",
            "This is a malformed line",
            "2023-10-27T10:03:00,ERROR,Database connection failed",
            "2023-10-27T10:04:00,INFO,User logged out",
            "2023-10-27T10:05:00,ERROR,Null pointer exception in module X"
        );
        ```

4.  **Analytics Service:**
    *   Create a `LogAnalytics` class. This is where your main stream pipeline will be.
    *   It should have a method `generateReport(List<String> rawLogs)` that returns a `Map<String, Long>`.
    *   **The Pipeline Logic:** Inside this method, build a stream pipeline that:
        1.  Starts with `rawLogs.stream()`.
        2.  Calls the `parseLogLine` method for each line. This will produce a `Stream<Optional<LogEntry>>`.
        3.  Filters out the empty `Optional`s to discard malformed lines.
        4.  "Unwraps" the `Optional`s to get a `Stream<LogEntry>`. (Hint: `opt -> opt.isPresent()` and `opt -> opt.get()` or a Java 9+ `Optional::stream`).
        5.  Uses a `groupingBy` collector to group the `LogEntry` objects by their log level (`LogEntry::level`).
        6.  Uses a downstream `counting` collector to count the number of entries in each group.
        7.  The final result will be a `Map<String, Long>` mapping each log level to its frequency.

5.  **Main Class:**
    *   The `main` method should create the `LogAnalytics` service, call `generateReport` with the sample data, and print the resulting map in a readable format.

**Expected Output:**
```
Log Level Summary:
- INFO: 3
- WARN: 1
- ERROR: 2
```

